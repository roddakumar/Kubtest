
==> Audit <==
|-----------|------------------------------------------------------------|----------|------------------|---------|---------------------|---------------------|
|  Command  |                            Args                            | Profile  |       User       | Version |     Start Time      |      End Time       |
|-----------|------------------------------------------------------------|----------|------------------|---------|---------------------|---------------------|
| kubectl   | -- apply -f service.yaml                                   | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 19:38 IST | 09 Oct 24 19:38 IST |
| service   | kubtest                                                    | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 19:39 IST |                     |
| service   | kubtest                                                    | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 19:39 IST |                     |
| service   | kubtest                                                    | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 19:46 IST |                     |
| service   | kubtest-sservice                                           | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 19:47 IST |                     |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 19:47 IST | 09 Oct 24 19:47 IST |
| kubectl   | -- apply -f service.yaml                                   | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:02 IST | 09 Oct 24 20:02 IST |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:03 IST | 09 Oct 24 20:03 IST |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:04 IST | 09 Oct 24 20:04 IST |
| stop      |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:05 IST |                     |
| dashboard |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:05 IST |                     |
| stop      |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:06 IST |                     |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:06 IST | 09 Oct 24 20:08 IST |
| dashboard |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:08 IST |                     |
| kubectl   | -- apply -f deployment.yaml                                | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:11 IST | 09 Oct 24 20:11 IST |
| kubectl   | -- apply -f service.yaml                                   | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:11 IST | 09 Oct 24 20:11 IST |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:12 IST |                     |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 09 Oct 24 20:14 IST | 09 Oct 24 20:14 IST |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:00 IST | 19 Oct 24 16:02 IST |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:09 IST |                     |
| stop      |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:15 IST | 19 Oct 24 16:16 IST |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:16 IST | 19 Oct 24 16:17 IST |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:23 IST | 19 Oct 24 16:24 IST |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:24 IST |                     |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:36 IST | 19 Oct 24 16:36 IST |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:36 IST |                     |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:39 IST | 19 Oct 24 16:40 IST |
| stop      |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:40 IST |                     |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:40 IST | 19 Oct 24 16:50 IST |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 16:57 IST |                     |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:09 IST |                     |
| tunnel    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:09 IST |                     |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:12 IST |                     |
| tunnel    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:13 IST |                     |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:20 IST |                     |
| tunnel    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:24 IST |                     |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:24 IST |                     |
| tunnel    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:34 IST |                     |
| service   | kuubtest-service                                           | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:44 IST |                     |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 17:44 IST |                     |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 19 Oct 24 18:06 IST | 19 Oct 24 18:06 IST |
| ip        |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:01 IST |                     |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:01 IST |                     |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:03 IST |                     |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:04 IST |                     |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:07 IST |                     |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:08 IST | 27 Oct 24 10:08 IST |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:08 IST | 27 Oct 24 10:10 IST |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:11 IST | 27 Oct 24 10:12 IST |
| start     | --image-repository=registry.aliyuncs.com/google_containers | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:13 IST | 27 Oct 24 10:14 IST |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:16 IST | 27 Oct 24 10:16 IST |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:16 IST | 27 Oct 24 10:18 IST |
| ip        |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:20 IST | 27 Oct 24 10:20 IST |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:22 IST |                     |
| delete    |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:23 IST | 27 Oct 24 10:23 IST |
| ip        |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:24 IST |                     |
| start     |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:24 IST | 27 Oct 24 10:26 IST |
| ip        |                                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:26 IST | 27 Oct 24 10:26 IST |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:26 IST |                     |
| service   | kubtest-service                                            | minikube | KUMARRODDA\rodda | v1.34.0 | 27 Oct 24 10:27 IST | 27 Oct 24 10:27 IST |
|-----------|------------------------------------------------------------|----------|------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/10/27 10:24:23
Running on machine: KumarRodda
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1027 10:24:23.456310    3660 out.go:345] Setting OutFile to fd 356 ...
I1027 10:24:23.457393    3660 out.go:392] TERM=,COLORTERM=, which probably does not support color
I1027 10:24:23.457393    3660 out.go:358] Setting ErrFile to fd 104...
I1027 10:24:23.457393    3660 out.go:392] TERM=,COLORTERM=, which probably does not support color
I1027 10:24:23.487084    3660 out.go:352] Setting JSON to false
I1027 10:24:23.495751    3660 start.go:129] hostinfo: {"hostname":"KumarRodda","uptime":1095,"bootTime":1730003768,"procs":323,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4317 Build 22631.4317","kernelVersion":"10.0.22631.4317 Build 22631.4317","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"57504bdb-7c60-4c74-ab60-c77e64300e5f"}
W1027 10:24:23.495751    3660 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1027 10:24:23.499522    3660 out.go:177] * minikube v1.34.0 on Microsoft Windows 11 Pro 10.0.22631.4317 Build 22631.4317
I1027 10:24:23.507182    3660 notify.go:220] Checking for updates...
I1027 10:24:23.508983    3660 driver.go:394] Setting default libvirt URI to qemu:///system
I1027 10:24:23.509536    3660 global.go:112] Querying for installed drivers using PATH=C:\Python312\Scripts\;C:\Python312\;C:\app\rodda\product\21c\dbhomeXE\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\dotnet\;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\nodejs\;C:\ProgramData\chocolatey\bin;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\160\Tools\Binn\;C:\Program Files\Microsoft SQL Server\160\Tools\Binn\;C:\Program Files\Microsoft SQL Server\160\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\160\DTS\Binn\;C:\Program Files\Azure Data Studio\bin;C:\Program Files\Git\cmd;C:\Python312\Scripts;C:\Users\rodda\AppData\Roaming\Python\Python312\Scripts;;C:\Program Files\Docker\Docker\resources\bin;C:\minikube;C:\Users\rodda\AppData\Local\Microsoft\WindowsApps;C:\Users\rodda\AppData\Roaming\npm;C:\Users\rodda\.dotnet\tools;C:\Users\rodda\AppData\Local\Programs\Microsoft VS Code\bin;C:\Program Files\Azure Data Studio\bin
I1027 10:24:23.509536    3660 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1027 10:24:24.895792    3660 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1027 10:24:24.903738    3660 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1027 10:24:24.920020    3660 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1027 10:24:24.927314    3660 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
W1027 10:24:24.991658    3660 docker.go:181] docker version returned error: exit status 1
I1027 10:24:24.991658    3660 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:false Running:true NeedsImprovement:false Error:"docker version --format {{.Server.Os}}-{{.Server.Version}}:{{.Server.Platform.Name}}" exit status 1: error during connect: Get "http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/v1.47/version": open //./pipe/dockerDesktopLinuxEngine: The system cannot find the file specified. Reason:PROVIDER_DOCKER_VERSION_EXIT_1 Fix: Doc:https://minikube.sigs.k8s.io/docs/drivers/docker/ Version:}
I1027 10:24:25.000864    3660 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1027 10:24:25.001395    3660 driver.go:316] not recommending "ssh" due to default: false
I1027 10:24:25.001395    3660 driver.go:311] not recommending "docker" due to health: "docker version --format {{.Server.Os}}-{{.Server.Version}}:{{.Server.Platform.Name}}" exit status 1: error during connect: Get "http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/v1.47/version": open //./pipe/dockerDesktopLinuxEngine: The system cannot find the file specified.
I1027 10:24:25.001395    3660 driver.go:351] Picked: hyperv
I1027 10:24:25.001395    3660 driver.go:352] Alternatives: [ssh]
I1027 10:24:25.001395    3660 driver.go:353] Rejects: [qemu2 virtualbox vmware docker podman]
I1027 10:24:25.003847    3660 out.go:177] * Automatically selected the hyperv driver
I1027 10:24:25.006911    3660 start.go:297] selected driver: hyperv
I1027 10:24:25.006911    3660 start.go:901] validating driver "hyperv" against <nil>
I1027 10:24:25.006911    3660 start.go:912] status for hyperv: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1027 10:24:25.007519    3660 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I1027 10:24:25.059876    3660 start_flags.go:393] Using suggested 4000MB memory alloc based on sys=16072MB, container=0MB
I1027 10:24:25.060390    3660 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I1027 10:24:25.062139    3660 cni.go:84] Creating CNI manager for ""
I1027 10:24:25.062222    3660 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1027 10:24:25.062222    3660 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1027 10:24:25.062889    3660 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rodda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1027 10:24:25.063397    3660 iso.go:125] acquiring lock: {Name:mkf874cba98637db066ed32517ad343e6eb651ab Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1027 10:24:25.067488    3660 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I1027 10:24:25.070946    3660 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1027 10:24:25.071056    3660 preload.go:146] Found local preload: C:\Users\rodda\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1027 10:24:25.071056    3660 cache.go:56] Caching tarball of preloaded images
I1027 10:24:25.071565    3660 preload.go:172] Found C:\Users\rodda\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1027 10:24:25.071565    3660 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1027 10:24:25.072206    3660 profile.go:143] Saving config to C:\Users\rodda\.minikube\profiles\minikube\config.json ...
I1027 10:24:25.072206    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.minikube\profiles\minikube\config.json: {Name:mkb42a006fd07d6ac5a818e564826cdf327f6695 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:24:25.072714    3660 start.go:360] acquireMachinesLock for minikube: {Name:mk52668bbe8f05192fea92bae4410216c9a9f1ab Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1027 10:24:25.073261    3660 start.go:364] duration metric: took 546.9µs to acquireMachinesLock for "minikube"
I1027 10:24:25.073344    3660 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rodda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1027 10:24:25.073344    3660 start.go:125] createHost starting for "" (driver="hyperv")
I1027 10:24:25.075531    3660 out.go:235] * Creating hyperv VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...
I1027 10:24:25.094248    3660 start.go:159] libmachine.API.Create for "minikube" (driver="hyperv")
I1027 10:24:25.094248    3660 client.go:168] LocalClient.Create starting
I1027 10:24:25.095264    3660 main.go:141] libmachine: Reading certificate data from C:\Users\rodda\.minikube\certs\ca.pem
I1027 10:24:25.096076    3660 main.go:141] libmachine: Decoding PEM data...
I1027 10:24:25.096076    3660 main.go:141] libmachine: Parsing certificate...
I1027 10:24:25.097252    3660 main.go:141] libmachine: Reading certificate data from C:\Users\rodda\.minikube\certs\cert.pem
I1027 10:24:25.097872    3660 main.go:141] libmachine: Decoding PEM data...
I1027 10:24:25.097872    3660 main.go:141] libmachine: Parsing certificate...
I1027 10:24:25.098419    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @(Get-Module -ListAvailable hyper-v).Name | Get-Unique
I1027 10:24:25.518603    3660 main.go:141] libmachine: [stdout =====>] : Hyper-V

I1027 10:24:25.518603    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:25.518603    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole(([System.Security.Principal.SecurityIdentifier]::new("S-1-5-32-578")))
I1027 10:24:25.726692    3660 main.go:141] libmachine: [stdout =====>] : True

I1027 10:24:25.726692    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:25.726692    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive [Console]::OutputEncoding = [Text.Encoding]::UTF8; ConvertTo-Json @(Hyper-V\Get-VMSwitch|Select Id, Name, SwitchType|Where-Object {($_.SwitchType -eq 'External') -or ($_.Id -eq 'c08cb7b8-9b3c-408e-8e30-5e16a3aeb444')}|Sort-Object -Property SwitchType)
I1027 10:24:27.107877    3660 main.go:141] libmachine: [stdout =====>] : [
    {
        "Id":  "c08cb7b8-9b3c-408e-8e30-5e16a3aeb444",
        "Name":  "Default Switch",
        "SwitchType":  1
    }
]

I1027 10:24:27.107877    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:27.108899    3660 main.go:141] libmachine: Downloading C:\Users\rodda\.minikube\cache\boot2docker.iso from file://C:/Users/rodda/.minikube/cache/iso/amd64/minikube-v1.34.0-amd64.iso...
I1027 10:24:27.397767    3660 main.go:141] libmachine: Creating SSH key...
I1027 10:24:27.554292    3660 main.go:141] libmachine: Creating VM...
I1027 10:24:27.554292    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive [Console]::OutputEncoding = [Text.Encoding]::UTF8; ConvertTo-Json @(Hyper-V\Get-VMSwitch|Select Id, Name, SwitchType|Where-Object {($_.SwitchType -eq 'External') -or ($_.Id -eq 'c08cb7b8-9b3c-408e-8e30-5e16a3aeb444')}|Sort-Object -Property SwitchType)
I1027 10:24:28.704747    3660 main.go:141] libmachine: [stdout =====>] : [
    {
        "Id":  "c08cb7b8-9b3c-408e-8e30-5e16a3aeb444",
        "Name":  "Default Switch",
        "SwitchType":  1
    }
]

I1027 10:24:28.704747    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:28.704747    3660 main.go:141] libmachine: Using switch "Default Switch"
I1027 10:24:28.704747    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")
I1027 10:24:28.992529    3660 main.go:141] libmachine: [stdout =====>] : False

I1027 10:24:28.992529    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:28.992529    3660 main.go:141] libmachine: Creating VHD
I1027 10:24:28.992661    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\New-VHD -Path 'C:\Users\rodda\.minikube\machines\minikube\fixed.vhd' -SizeBytes 20000MB -Fixed
I1027 10:24:34.888277    3660 main.go:141] libmachine: [stdout =====>] : 

ComputerName            : KUMARRODDA
Path                    : C:\Users\rodda\.minikube\machines\minikube\fixed.vhd
VhdFormat               : VHD
VhdType                 : Fixed
FileSize                : 20971520512
Size                    : 20971520000
MinimumSize             : 
LogicalSectorSize       : 512
PhysicalSectorSize      : 512
BlockSize               : 0
ParentPath              : 
DiskIdentifier          : F5519532-8FAD-4B60-94AD-24284EC2BF6F
FragmentationPercentage : 0
Alignment               : 1
Attached                : False
DiskNumber              : 
IsPMEMCompatible        : False
AddressAbstractionType  : None
Number                  : 




I1027 10:24:34.888277    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:34.888277    3660 main.go:141] libmachine: Writing magic tar header
I1027 10:24:34.889126    3660 main.go:141] libmachine: Writing SSH key tar header
I1027 10:24:34.900530    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Convert-VHD -Path 'C:\Users\rodda\.minikube\machines\minikube\fixed.vhd' -DestinationPath 'C:\Users\rodda\.minikube\machines\minikube\disk.vhd' -VHDType Dynamic -DeleteSource
I1027 10:24:42.824550    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:42.824550    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:42.824550    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\New-VM minikube -Path 'C:\Users\rodda\.minikube\machines\minikube' -SwitchName 'Default Switch' -MemoryStartupBytes 4000MB
I1027 10:24:47.228933    3660 main.go:141] libmachine: [stdout =====>] : 
Name     State CPUUsage(%) MemoryAssigned(M) Uptime   Status             Version
----     ----- ----------- ----------------- ------   ------             -------
minikube Off   0           0                 00:00:00 Operating normally 11.0   



I1027 10:24:47.228933    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:47.228933    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMMemory -VMName minikube -DynamicMemoryEnabled $false
I1027 10:24:48.271444    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:48.271444    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:48.271989    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMProcessor minikube -Count 2
I1027 10:24:49.430524    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:49.431468    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:49.431468    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMDvdDrive -VMName minikube -Path 'C:\Users\rodda\.minikube\machines\minikube\boot2docker.iso'
I1027 10:24:50.997284    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:50.997284    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:50.997284    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Add-VMHardDiskDrive -VMName minikube -Path 'C:\Users\rodda\.minikube\machines\minikube\disk.vhd'
I1027 10:24:52.394475    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:52.394475    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:52.394475    3660 main.go:141] libmachine: Starting VM...
I1027 10:24:52.394475    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Start-VM minikube
I1027 10:24:54.536933    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:54.536933    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:54.536933    3660 main.go:141] libmachine: Waiting for host to start...
I1027 10:24:54.536933    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:24:55.326124    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:24:55.326124    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:55.326124    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:24:56.630770    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:56.630770    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:57.643450    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:24:58.239439    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:24:58.239439    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:24:58.239439    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:24:59.228585    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:24:59.228585    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:00.236239    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:00.927836    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:00.927836    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:00.927836    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:02.074750    3660 main.go:141] libmachine: [stdout =====>] : 
I1027 10:25:02.074750    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:03.084704    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:03.933784    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:03.933784    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:03.934127    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:05.348136    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:05.348136    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:05.348136    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:06.184417    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:06.184417    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:06.186343    3660 machine.go:93] provisionDockerMachine start ...
I1027 10:25:06.186343    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:06.927679    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:06.927679    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:06.927679    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:08.373571    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:08.373571    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:08.378243    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:08.395737    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:08.395737    3660 main.go:141] libmachine: About to run SSH command:
hostname
I1027 10:25:08.523657    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1027 10:25:08.524753    3660 buildroot.go:166] provisioning hostname "minikube"
I1027 10:25:08.524753    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:09.379763    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:09.379763    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:09.379763    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:10.594830    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:10.594830    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:10.597881    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:10.597881    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:10.597881    3660 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1027 10:25:10.755545    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1027 10:25:10.757157    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:11.650258    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:11.650258    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:11.650258    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:13.041591    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:13.041591    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:13.046686    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:13.047086    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:13.047086    3660 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1027 10:25:13.192859    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1027 10:25:13.194378    3660 buildroot.go:172] set auth options {CertDir:C:\Users\rodda\.minikube CaCertPath:C:\Users\rodda\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\rodda\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\rodda\.minikube\machines\server.pem ServerKeyPath:C:\Users\rodda\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\rodda\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\rodda\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\rodda\.minikube}
I1027 10:25:13.194378    3660 buildroot.go:174] setting up certificates
I1027 10:25:13.194889    3660 provision.go:84] configureAuth start
I1027 10:25:13.194889    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:14.109981    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:14.109981    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:14.109981    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:15.470583    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:15.470583    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:15.471122    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:16.399109    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:16.399109    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:16.399768    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:17.758646    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:17.758646    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:17.759156    3660 provision.go:143] copyHostCerts
I1027 10:25:17.760472    3660 exec_runner.go:144] found C:\Users\rodda\.minikube/ca.pem, removing ...
I1027 10:25:17.761122    3660 exec_runner.go:203] rm: C:\Users\rodda\.minikube\ca.pem
I1027 10:25:17.761122    3660 exec_runner.go:151] cp: C:\Users\rodda\.minikube\certs\ca.pem --> C:\Users\rodda\.minikube/ca.pem (1074 bytes)
I1027 10:25:17.761728    3660 exec_runner.go:144] found C:\Users\rodda\.minikube/cert.pem, removing ...
I1027 10:25:17.761728    3660 exec_runner.go:203] rm: C:\Users\rodda\.minikube\cert.pem
I1027 10:25:17.762235    3660 exec_runner.go:151] cp: C:\Users\rodda\.minikube\certs\cert.pem --> C:\Users\rodda\.minikube/cert.pem (1119 bytes)
I1027 10:25:17.763335    3660 exec_runner.go:144] found C:\Users\rodda\.minikube/key.pem, removing ...
I1027 10:25:17.763335    3660 exec_runner.go:203] rm: C:\Users\rodda\.minikube\key.pem
I1027 10:25:17.763335    3660 exec_runner.go:151] cp: C:\Users\rodda\.minikube\certs\key.pem --> C:\Users\rodda\.minikube/key.pem (1675 bytes)
I1027 10:25:17.763871    3660 provision.go:117] generating server cert: C:\Users\rodda\.minikube\machines\server.pem ca-key=C:\Users\rodda\.minikube\certs\ca.pem private-key=C:\Users\rodda\.minikube\certs\ca-key.pem org=rodda.minikube san=[127.0.0.1 172.28.146.145 localhost minikube]
I1027 10:25:17.851338    3660 provision.go:177] copyRemoteCerts
I1027 10:25:17.864377    3660 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1027 10:25:17.864377    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:18.697938    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:18.697938    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:18.698672    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:20.215749    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:20.216521    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:20.216521    3660 sshutil.go:53] new ssh client: &{IP:172.28.146.145 Port:22 SSHKeyPath:C:\Users\rodda\.minikube\machines\minikube\id_rsa Username:docker}
I1027 10:25:20.319300    3660 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (2.4549232s)
I1027 10:25:20.320233    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1027 10:25:20.340701    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1027 10:25:20.357155    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1027 10:25:20.374697    3660 provision.go:87] duration metric: took 7.1798082s to configureAuth
I1027 10:25:20.374697    3660 buildroot.go:189] setting minikube options for container-runtime
I1027 10:25:20.375210    3660 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1027 10:25:20.375210    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:21.025835    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:21.025835    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:21.026503    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:22.206188    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:22.206188    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:22.208966    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:22.209135    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:22.209135    3660 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1027 10:25:22.337588    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I1027 10:25:22.337588    3660 buildroot.go:70] root file system type: tmpfs
I1027 10:25:22.338101    3660 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1027 10:25:22.338101    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:23.261581    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:23.261581    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:23.261581    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:24.624077    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:24.624077    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:24.626252    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:24.626865    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:24.626865    3660 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1027 10:25:24.772144    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1027 10:25:24.773888    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:25.704379    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:25.704379    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:25.704903    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:27.126817    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:27.126817    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:27.132126    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:27.132365    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:27.132365    3660 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1027 10:25:28.996613    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I1027 10:25:28.996613    3660 machine.go:96] duration metric: took 22.8102697s to provisionDockerMachine
I1027 10:25:28.996613    3660 client.go:171] duration metric: took 1m3.9023651s to LocalClient.Create
I1027 10:25:28.996613    3660 start.go:167] duration metric: took 1m3.9023651s to libmachine.API.Create "minikube"
I1027 10:25:28.997201    3660 start.go:293] postStartSetup for "minikube" (driver="hyperv")
I1027 10:25:28.997201    3660 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1027 10:25:29.010239    3660 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1027 10:25:29.010239    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:29.695885    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:29.695885    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:29.695885    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:31.140850    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:31.140850    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:31.141242    3660 sshutil.go:53] new ssh client: &{IP:172.28.146.145 Port:22 SSHKeyPath:C:\Users\rodda\.minikube\machines\minikube\id_rsa Username:docker}
I1027 10:25:31.262474    3660 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (2.2522351s)
I1027 10:25:31.277375    3660 ssh_runner.go:195] Run: cat /etc/os-release
I1027 10:25:31.285862    3660 info.go:137] Remote host: Buildroot 2023.02.9
I1027 10:25:31.286219    3660 filesync.go:126] Scanning C:\Users\rodda\.minikube\addons for local assets ...
I1027 10:25:31.286577    3660 filesync.go:126] Scanning C:\Users\rodda\.minikube\files for local assets ...
I1027 10:25:31.286748    3660 start.go:296] duration metric: took 2.2893767s for postStartSetup
I1027 10:25:31.288426    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:32.210541    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:32.210541    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:32.210541    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:33.446188    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:33.446188    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:33.446851    3660 profile.go:143] Saving config to C:\Users\rodda\.minikube\profiles\minikube\config.json ...
I1027 10:25:33.450365    3660 start.go:128] duration metric: took 1m8.3762253s to createHost
I1027 10:25:33.450365    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:34.310098    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:34.310098    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:34.310994    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:35.601834    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:35.601834    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:35.604843    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:35.605403    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:35.605403    3660 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I1027 10:25:35.740492    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: 1730004936.956224012

I1027 10:25:35.740492    3660 fix.go:216] guest clock: 1730004936.956224012
I1027 10:25:35.740492    3660 fix.go:229] Guest: 2024-10-27 10:25:36.956224012 +0530 IST Remote: 2024-10-27 10:25:33.4503653 +0530 IST m=+70.098996001 (delta=3.505858712s)
I1027 10:25:35.742360    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:36.638615    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:36.638615    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:36.638615    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:37.930928    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:37.930928    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:37.935857    3660 main.go:141] libmachine: Using SSH client type: native
I1027 10:25:37.936664    3660 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xcfc9c0] 0xcff5a0 <nil>  [] 0s} 172.28.146.145 22 <nil> <nil>}
I1027 10:25:37.936664    3660 main.go:141] libmachine: About to run SSH command:
sudo date -s @1730004935
I1027 10:25:38.084794    3660 main.go:141] libmachine: SSH cmd err, output: <nil>: Sun Oct 27 04:55:35 UTC 2024

I1027 10:25:38.084794    3660 fix.go:236] clock set: Sun Oct 27 04:55:35 UTC 2024
 (err=<nil>)
I1027 10:25:38.084794    3660 start.go:83] releasing machines lock for "minikube", held for 1m13.0115334s
I1027 10:25:38.085302    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:38.908324    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:38.908324    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:38.908324    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:40.263236    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:40.263236    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:40.269633    3660 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1027 10:25:40.270174    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:40.276922    3660 ssh_runner.go:195] Run: cat /version.json
I1027 10:25:40.276922    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:25:41.229712    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:41.229712    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:41.230738    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:41.265667    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:25:41.265667    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:41.265925    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:25:42.503286    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:42.503416    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:42.503416    3660 sshutil.go:53] new ssh client: &{IP:172.28.146.145 Port:22 SSHKeyPath:C:\Users\rodda\.minikube\machines\minikube\id_rsa Username:docker}
I1027 10:25:42.521687    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:25:42.521687    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:25:42.521803    3660 sshutil.go:53] new ssh client: &{IP:172.28.146.145 Port:22 SSHKeyPath:C:\Users\rodda\.minikube\machines\minikube\id_rsa Username:docker}
I1027 10:25:42.586513    3660 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (2.31688s)
W1027 10:25:42.586513    3660 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1027 10:25:42.615245    3660 ssh_runner.go:235] Completed: cat /version.json: (2.3381576s)
I1027 10:25:42.631036    3660 ssh_runner.go:195] Run: systemctl --version
I1027 10:25:42.651468    3660 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1027 10:25:42.657202    3660 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I1027 10:25:42.668083    3660 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1027 10:25:42.689659    3660 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1027 10:25:42.689659    3660 start.go:495] detecting cgroup driver to use...
I1027 10:25:42.692185    3660 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1027 10:25:42.732310    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1027 10:25:42.757883    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1027 10:25:42.771198    3660 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1027 10:25:42.783632    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1027 10:25:42.810911    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1027 10:25:42.842047    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1027 10:25:42.869753    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1027 10:25:42.899177    3660 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1027 10:25:42.931124    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
W1027 10:25:42.956434    3660 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube VM
W1027 10:25:42.957330    3660 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1027 10:25:42.962229    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1027 10:25:42.993659    3660 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1027 10:25:43.023186    3660 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1027 10:25:43.044027    3660 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1027 10:25:43.067487    3660 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1027 10:25:43.171075    3660 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1027 10:25:43.190343    3660 start.go:495] detecting cgroup driver to use...
I1027 10:25:43.199976    3660 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1027 10:25:43.216250    3660 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1027 10:25:43.232137    3660 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1027 10:25:43.254186    3660 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1027 10:25:43.277417    3660 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1027 10:25:43.305350    3660 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I1027 10:25:43.363696    3660 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1027 10:25:43.387196    3660 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1027 10:25:43.431636    3660 ssh_runner.go:195] Run: which cri-dockerd
I1027 10:25:43.450949    3660 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1027 10:25:43.465851    3660 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1027 10:25:43.497715    3660 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1027 10:25:43.601353    3660 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1027 10:25:43.681682    3660 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1027 10:25:43.683429    3660 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1027 10:25:43.702037    3660 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1027 10:25:43.782505    3660 ssh_runner.go:195] Run: sudo systemctl restart docker
I1027 10:25:46.122277    3660 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.3397724s)
I1027 10:25:46.134685    3660 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1027 10:25:46.156095    3660 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1027 10:25:46.173855    3660 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1027 10:25:46.296228    3660 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1027 10:25:46.384533    3660 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1027 10:25:46.471840    3660 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1027 10:25:46.491434    3660 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1027 10:25:46.509425    3660 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1027 10:25:46.604048    3660 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1027 10:25:46.647231    3660 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1027 10:25:46.654788    3660 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1027 10:25:46.657985    3660 start.go:563] Will wait 60s for crictl version
I1027 10:25:46.664305    3660 ssh_runner.go:195] Run: which crictl
I1027 10:25:46.673641    3660 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1027 10:25:46.696943    3660 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1027 10:25:46.702438    3660 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1027 10:25:46.723052    3660 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1027 10:25:46.748294    3660 out.go:235] * Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1027 10:25:46.750796    3660 ip.go:176] getIPForInterface: searching for "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:190] "Ethernet" does not match prefix "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:190] "Wi-Fi 2" does not match prefix "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:190] "Wi-Fi 3" does not match prefix "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:190] "Wi-Fi 5" does not match prefix "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:190] "Wi-Fi" does not match prefix "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:190] "Loopback Pseudo-Interface 1" does not match prefix "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:185] found prefix matching interface for "vEthernet (Default Switch)": "vEthernet (Default Switch)"
I1027 10:25:46.759484    3660 ip.go:211] Found interface: {Index:24 MTU:1500 Name:vEthernet (Default Switch) HardwareAddr:00:15:5d:1c:55:38 Flags:up|broadcast|multicast|running}
I1027 10:25:46.764203    3660 ip.go:214] interface addr: fe80::f169:ee10:6e8:53f0/64
I1027 10:25:46.764203    3660 ip.go:214] interface addr: 172.28.144.1/20
I1027 10:25:46.773711    3660 ssh_runner.go:195] Run: grep 172.28.144.1	host.minikube.internal$ /etc/hosts
I1027 10:25:46.777999    3660 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "172.28.144.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1027 10:25:46.792655    3660 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.28.146.145 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rodda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1027 10:25:46.793165    3660 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1027 10:25:46.803148    3660 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1027 10:25:46.816247    3660 docker.go:685] Got preloaded images: 
I1027 10:25:46.816247    3660 docker.go:691] registry.k8s.io/kube-apiserver:v1.31.0 wasn't preloaded
I1027 10:25:46.822625    3660 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I1027 10:25:46.837546    3660 ssh_runner.go:195] Run: which lz4
I1027 10:25:46.845970    3660 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I1027 10:25:46.848614    3660 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I1027 10:25:46.848923    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (342554258 bytes)
I1027 10:25:47.536170    3660 docker.go:649] duration metric: took 696.2153ms to copy over tarball
I1027 10:25:47.542741    3660 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I1027 10:25:49.192565    3660 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (1.6496835s)
I1027 10:25:49.192565    3660 ssh_runner.go:146] rm: /preloaded.tar.lz4
I1027 10:25:49.226202    3660 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I1027 10:25:49.232801    3660 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I1027 10:25:49.260358    3660 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1027 10:25:49.368193    3660 ssh_runner.go:195] Run: sudo systemctl restart docker
I1027 10:25:51.778458    3660 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.4102652s)
I1027 10:25:51.786639    3660 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1027 10:25:51.800095    3660 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1027 10:25:51.800095    3660 cache_images.go:84] Images are preloaded, skipping loading
I1027 10:25:51.800095    3660 kubeadm.go:934] updating node { 172.28.146.145 8443 v1.31.0 docker true true} ...
I1027 10:25:51.801121    3660 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.28.146.145

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1027 10:25:51.806983    3660 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1027 10:25:51.838990    3660 cni.go:84] Creating CNI manager for ""
I1027 10:25:51.838990    3660 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1027 10:25:51.838990    3660 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1027 10:25:51.838990    3660 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:172.28.146.145 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "172.28.146.145"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:172.28.146.145 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1027 10:25:51.838990    3660 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.28.146.145
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 172.28.146.145
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "172.28.146.145"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1027 10:25:51.846262    3660 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1027 10:25:51.853757    3660 binaries.go:44] Found k8s binaries, skipping transfer
I1027 10:25:51.860374    3660 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1027 10:25:51.867423    3660 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (309 bytes)
I1027 10:25:51.878626    3660 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1027 10:25:51.892652    3660 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2156 bytes)
I1027 10:25:51.912032    3660 ssh_runner.go:195] Run: grep 172.28.146.145	control-plane.minikube.internal$ /etc/hosts
I1027 10:25:51.914695    3660 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "172.28.146.145	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1027 10:25:51.929616    3660 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1027 10:25:52.017975    3660 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1027 10:25:52.030846    3660 certs.go:68] Setting up C:\Users\rodda\.minikube\profiles\minikube for IP: 172.28.146.145
I1027 10:25:52.030846    3660 certs.go:194] generating shared ca certs ...
I1027 10:25:52.030846    3660 certs.go:226] acquiring lock for ca certs: {Name:mk0aff53ace1b1b549e926736cb1793a7ddeb72d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:25:52.031368    3660 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\rodda\.minikube\ca.key
I1027 10:25:52.031911    3660 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\rodda\.minikube\proxy-client-ca.key
I1027 10:25:52.031911    3660 certs.go:256] generating profile certs ...
I1027 10:25:52.032434    3660 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\rodda\.minikube\profiles\minikube\client.key
I1027 10:25:52.032715    3660 crypto.go:68] Generating cert C:\Users\rodda\.minikube\profiles\minikube\client.crt with IP's: []
I1027 10:25:52.138801    3660 crypto.go:156] Writing cert to C:\Users\rodda\.minikube\profiles\minikube\client.crt ...
I1027 10:25:52.138801    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.minikube\profiles\minikube\client.crt: {Name:mk4743d4eeeae599c5970a10e377f23fa1a2fd51 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:25:52.138801    3660 crypto.go:164] Writing key to C:\Users\rodda\.minikube\profiles\minikube\client.key ...
I1027 10:25:52.138801    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.minikube\profiles\minikube\client.key: {Name:mk54781ca5f8f3cf76aabc9a17ba8176767fcc95 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:25:52.139839    3660 certs.go:363] generating signed profile cert for "minikube": C:\Users\rodda\.minikube\profiles\minikube\apiserver.key.e0ada25c
I1027 10:25:52.139839    3660 crypto.go:68] Generating cert C:\Users\rodda\.minikube\profiles\minikube\apiserver.crt.e0ada25c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 172.28.146.145]
I1027 10:25:52.229026    3660 crypto.go:156] Writing cert to C:\Users\rodda\.minikube\profiles\minikube\apiserver.crt.e0ada25c ...
I1027 10:25:52.229026    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.minikube\profiles\minikube\apiserver.crt.e0ada25c: {Name:mkfdc7090586a493681d16044a1579d802ed3112 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:25:52.229972    3660 crypto.go:164] Writing key to C:\Users\rodda\.minikube\profiles\minikube\apiserver.key.e0ada25c ...
I1027 10:25:52.229972    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.minikube\profiles\minikube\apiserver.key.e0ada25c: {Name:mkd7f6e4f424b0117406e9acb4be6d60f468622e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:25:52.229972    3660 certs.go:381] copying C:\Users\rodda\.minikube\profiles\minikube\apiserver.crt.e0ada25c -> C:\Users\rodda\.minikube\profiles\minikube\apiserver.crt
I1027 10:25:52.245159    3660 certs.go:385] copying C:\Users\rodda\.minikube\profiles\minikube\apiserver.key.e0ada25c -> C:\Users\rodda\.minikube\profiles\minikube\apiserver.key
I1027 10:25:52.250162    3660 certs.go:363] generating signed profile cert for "aggregator": C:\Users\rodda\.minikube\profiles\minikube\proxy-client.key
I1027 10:25:52.250162    3660 crypto.go:68] Generating cert C:\Users\rodda\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I1027 10:25:52.364453    3660 crypto.go:156] Writing cert to C:\Users\rodda\.minikube\profiles\minikube\proxy-client.crt ...
I1027 10:25:52.364453    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.minikube\profiles\minikube\proxy-client.crt: {Name:mk73a23ef1e857dca0cbffc52f0dec7557d21c0f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:25:52.365297    3660 crypto.go:164] Writing key to C:\Users\rodda\.minikube\profiles\minikube\proxy-client.key ...
I1027 10:25:52.365297    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.minikube\profiles\minikube\proxy-client.key: {Name:mk7bf1508a209ee24f049ec6c7fa77230e338aa9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:25:52.380294    3660 certs.go:484] found cert: C:\Users\rodda\.minikube\certs\ca-key.pem (1675 bytes)
I1027 10:25:52.380294    3660 certs.go:484] found cert: C:\Users\rodda\.minikube\certs\ca.pem (1074 bytes)
I1027 10:25:52.380798    3660 certs.go:484] found cert: C:\Users\rodda\.minikube\certs\cert.pem (1119 bytes)
I1027 10:25:52.380860    3660 certs.go:484] found cert: C:\Users\rodda\.minikube\certs\key.pem (1675 bytes)
I1027 10:25:52.384904    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1027 10:25:52.410300    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1027 10:25:52.438246    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1027 10:25:52.458232    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1027 10:25:52.475400    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1027 10:25:52.492708    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1027 10:25:52.508511    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1027 10:25:52.525036    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1027 10:25:52.542061    3660 ssh_runner.go:362] scp C:\Users\rodda\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1027 10:25:52.558618    3660 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1027 10:25:52.576469    3660 ssh_runner.go:195] Run: openssl version
I1027 10:25:52.588278    3660 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1027 10:25:52.602599    3660 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1027 10:25:52.605797    3660 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct  9 05:35 /usr/share/ca-certificates/minikubeCA.pem
I1027 10:25:52.614462    3660 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1027 10:25:52.629911    3660 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1027 10:25:52.650829    3660 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1027 10:25:52.654808    3660 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1027 10:25:52.654808    3660 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.28.146.145 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rodda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1027 10:25:52.661672    3660 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1027 10:25:52.679275    3660 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1027 10:25:52.698080    3660 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1027 10:25:52.712554    3660 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1027 10:25:52.719704    3660 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1027 10:25:52.719704    3660 kubeadm.go:157] found existing configuration files:

I1027 10:25:52.726108    3660 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1027 10:25:52.733204    3660 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1027 10:25:52.739258    3660 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1027 10:25:52.753775    3660 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1027 10:25:52.760009    3660 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1027 10:25:52.766936    3660 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1027 10:25:52.780481    3660 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1027 10:25:52.787751    3660 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1027 10:25:52.794643    3660 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1027 10:25:52.808539    3660 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1027 10:25:52.814996    3660 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1027 10:25:52.821291    3660 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1027 10:25:52.828136    3660 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I1027 10:25:52.856145    3660 kubeadm.go:310] W1027 04:55:54.071057    1723 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1027 10:25:52.856650    3660 kubeadm.go:310] W1027 04:55:54.072348    1723 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1027 10:25:52.913111    3660 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1027 10:26:01.021943    3660 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I1027 10:26:01.021943    3660 kubeadm.go:310] [preflight] Running pre-flight checks
I1027 10:26:01.022461    3660 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1027 10:26:01.022461    3660 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1027 10:26:01.023082    3660 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1027 10:26:01.023082    3660 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1027 10:26:01.025505    3660 out.go:235]   - Generating certificates and keys ...
I1027 10:26:01.028145    3660 kubeadm.go:310] [certs] Using existing ca certificate authority
I1027 10:26:01.028145    3660 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1027 10:26:01.028145    3660 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1027 10:26:01.028652    3660 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1027 10:26:01.028720    3660 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1027 10:26:01.028720    3660 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1027 10:26:01.028720    3660 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1027 10:26:01.029227    3660 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [172.28.146.145 127.0.0.1 ::1]
I1027 10:26:01.029227    3660 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1027 10:26:01.029227    3660 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [172.28.146.145 127.0.0.1 ::1]
I1027 10:26:01.029757    3660 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1027 10:26:01.029757    3660 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1027 10:26:01.029757    3660 kubeadm.go:310] [certs] Generating "sa" key and public key
I1027 10:26:01.029757    3660 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1027 10:26:01.030299    3660 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1027 10:26:01.030299    3660 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1027 10:26:01.030299    3660 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1027 10:26:01.030299    3660 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1027 10:26:01.030824    3660 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1027 10:26:01.030890    3660 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1027 10:26:01.030890    3660 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1027 10:26:01.034344    3660 out.go:235]   - Booting up control plane ...
I1027 10:26:01.035393    3660 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1027 10:26:01.035453    3660 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1027 10:26:01.035453    3660 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1027 10:26:01.035453    3660 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1027 10:26:01.035453    3660 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1027 10:26:01.035453    3660 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1027 10:26:01.035960    3660 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1027 10:26:01.035960    3660 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1027 10:26:01.035960    3660 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.774503ms
I1027 10:26:01.035960    3660 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1027 10:26:01.035960    3660 kubeadm.go:310] [api-check] The API server is healthy after 4.502560219s
I1027 10:26:01.035960    3660 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1027 10:26:01.036501    3660 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1027 10:26:01.036501    3660 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1027 10:26:01.036501    3660 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1027 10:26:01.036501    3660 kubeadm.go:310] [bootstrap-token] Using token: cgro6x.fsx2pecrzyoqswy5
I1027 10:26:01.038833    3660 out.go:235]   - Configuring RBAC rules ...
I1027 10:26:01.039439    3660 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1027 10:26:01.039972    3660 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1027 10:26:01.040077    3660 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1027 10:26:01.040077    3660 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1027 10:26:01.040077    3660 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1027 10:26:01.040077    3660 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1027 10:26:01.040584    3660 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1027 10:26:01.040717    3660 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1027 10:26:01.040717    3660 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1027 10:26:01.040717    3660 kubeadm.go:310] 
I1027 10:26:01.040717    3660 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1027 10:26:01.040717    3660 kubeadm.go:310] 
I1027 10:26:01.040717    3660 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1027 10:26:01.040717    3660 kubeadm.go:310] 
I1027 10:26:01.040717    3660 kubeadm.go:310]   mkdir -p $HOME/.kube
I1027 10:26:01.040717    3660 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1027 10:26:01.040717    3660 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1027 10:26:01.040717    3660 kubeadm.go:310] 
I1027 10:26:01.041246    3660 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1027 10:26:01.041246    3660 kubeadm.go:310] 
I1027 10:26:01.041373    3660 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1027 10:26:01.041373    3660 kubeadm.go:310] 
I1027 10:26:01.041373    3660 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1027 10:26:01.041373    3660 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1027 10:26:01.041373    3660 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1027 10:26:01.041373    3660 kubeadm.go:310] 
I1027 10:26:01.041373    3660 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1027 10:26:01.041879    3660 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1027 10:26:01.041879    3660 kubeadm.go:310] 
I1027 10:26:01.041879    3660 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token cgro6x.fsx2pecrzyoqswy5 \
I1027 10:26:01.041879    3660 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c9e88be2fbea33a936c0f06e3b9fac2ec1eeb6685d419530b53a1d7182131497 \
I1027 10:26:01.041879    3660 kubeadm.go:310] 	--control-plane 
I1027 10:26:01.041879    3660 kubeadm.go:310] 
I1027 10:26:01.041879    3660 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1027 10:26:01.041879    3660 kubeadm.go:310] 
I1027 10:26:01.042433    3660 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token cgro6x.fsx2pecrzyoqswy5 \
I1027 10:26:01.042433    3660 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c9e88be2fbea33a936c0f06e3b9fac2ec1eeb6685d419530b53a1d7182131497 
I1027 10:26:01.042433    3660 cni.go:84] Creating CNI manager for ""
I1027 10:26:01.042433    3660 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1027 10:26:01.044624    3660 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I1027 10:26:01.060661    3660 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1027 10:26:01.076698    3660 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1027 10:26:01.102838    3660 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1027 10:26:01.116649    3660 ops.go:34] apiserver oom_adj: -16
I1027 10:26:01.118896    3660 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1027 10:26:01.119443    3660 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_10_27T10_26_01_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1027 10:26:01.206735    3660 kubeadm.go:1113] duration metric: took 103.3827ms to wait for elevateKubeSystemPrivileges
I1027 10:26:01.215710    3660 kubeadm.go:394] duration metric: took 8.5609015s to StartCluster
I1027 10:26:01.215710    3660 settings.go:142] acquiring lock: {Name:mk03325d6965a08acb3d8c4d6abd7db04a47bf42 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:26:01.216450    3660 settings.go:150] Updating kubeconfig:  C:\Users\rodda\.kube\config
I1027 10:26:01.217512    3660 lock.go:35] WriteFile acquiring C:\Users\rodda\.kube\config: {Name:mk24424e2a53f7132554e2d1122ba75b996bb8c2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1027 10:26:01.219035    3660 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1027 10:26:01.219035    3660 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1027 10:26:01.219035    3660 start.go:235] Will wait 6m0s for node &{Name: IP:172.28.146.145 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1027 10:26:01.220187    3660 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1027 10:26:01.220187    3660 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1027 10:26:01.220187    3660 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1027 10:26:01.220187    3660 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I1027 10:26:01.220187    3660 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1027 10:26:01.221709    3660 host.go:66] Checking if "minikube" exists ...
I1027 10:26:01.221709    3660 out.go:177] * Verifying Kubernetes components...
I1027 10:26:01.222240    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:26:01.222240    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:26:01.236396    3660 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1027 10:26:01.318586    3660 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           172.28.144.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1027 10:26:01.359443    3660 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1027 10:26:01.494414    3660 start.go:971] {"host.minikube.internal": 172.28.144.1} host record injected into CoreDNS's ConfigMap
I1027 10:26:01.508884    3660 api_server.go:52] waiting for apiserver process to appear ...
I1027 10:26:01.516344    3660 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1027 10:26:01.525706    3660 api_server.go:72] duration metric: took 306.0858ms to wait for apiserver process to appear ...
I1027 10:26:01.525706    3660 api_server.go:88] waiting for apiserver healthz status ...
I1027 10:26:01.525706    3660 api_server.go:253] Checking apiserver healthz at https://172.28.146.145:8443/healthz ...
I1027 10:26:01.530500    3660 api_server.go:279] https://172.28.146.145:8443/healthz returned 200:
ok
I1027 10:26:01.531282    3660 api_server.go:141] control plane version: v1.31.0
I1027 10:26:01.531282    3660 api_server.go:131] duration metric: took 5.5767ms to wait for apiserver health ...
I1027 10:26:01.531282    3660 system_pods.go:43] waiting for kube-system pods to appear ...
I1027 10:26:01.540477    3660 system_pods.go:59] 4 kube-system pods found
I1027 10:26:01.540477    3660 system_pods.go:61] "etcd-minikube" [294b96e3-fda8-4be6-acb1-cad246957f76] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1027 10:26:01.540477    3660 system_pods.go:61] "kube-apiserver-minikube" [2e6e36d0-b5c0-4dec-a427-97b02e7ffd06] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1027 10:26:01.540477    3660 system_pods.go:61] "kube-controller-manager-minikube" [95031870-3b87-49e5-b840-0c45ec4c58d4] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1027 10:26:01.540477    3660 system_pods.go:61] "kube-scheduler-minikube" [49bb2edb-c7b6-4fdb-81f3-b41094f1bd1f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1027 10:26:01.540477    3660 system_pods.go:74] duration metric: took 9.1942ms to wait for pod list to return data ...
I1027 10:26:01.540477    3660 kubeadm.go:582] duration metric: took 320.8567ms to wait for: map[apiserver:true system_pods:true]
I1027 10:26:01.540477    3660 node_conditions.go:102] verifying NodePressure condition ...
I1027 10:26:01.543293    3660 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I1027 10:26:01.543293    3660 node_conditions.go:123] node cpu capacity is 2
I1027 10:26:01.543293    3660 node_conditions.go:105] duration metric: took 2.816ms to run NodePressure ...
I1027 10:26:01.543293    3660 start.go:241] waiting for startup goroutines ...
I1027 10:26:01.931478    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:26:01.931478    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:26:01.932910    3660 addons.go:234] Setting addon default-storageclass=true in "minikube"
I1027 10:26:01.933019    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:26:01.933019    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:26:01.933019    3660 host.go:66] Checking if "minikube" exists ...
I1027 10:26:01.933417    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:26:01.935445    3660 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1027 10:26:01.950862    3660 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1027 10:26:01.950862    3660 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1027 10:26:01.950862    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:26:02.023617    3660 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1027 10:26:02.969705    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:26:02.969705    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:26:02.969705    3660 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1027 10:26:02.969705    3660 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1027 10:26:02.970451    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1027 10:26:02.972850    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:26:02.972850    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:26:02.972850    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:26:03.856441    3660 main.go:141] libmachine: [stdout =====>] : Running

I1027 10:26:03.856441    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:26:03.856441    3660 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1027 10:26:04.274221    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:26:04.274221    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:26:04.274221    3660 sshutil.go:53] new ssh client: &{IP:172.28.146.145 Port:22 SSHKeyPath:C:\Users\rodda\.minikube\machines\minikube\id_rsa Username:docker}
I1027 10:26:04.404107    3660 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1027 10:26:05.101583    3660 main.go:141] libmachine: [stdout =====>] : 172.28.146.145

I1027 10:26:05.101583    3660 main.go:141] libmachine: [stderr =====>] : 
I1027 10:26:05.101583    3660 sshutil.go:53] new ssh client: &{IP:172.28.146.145 Port:22 SSHKeyPath:C:\Users\rodda\.minikube\machines\minikube\id_rsa Username:docker}
I1027 10:26:05.200783    3660 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1027 10:26:05.308340    3660 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I1027 10:26:05.315498    3660 addons.go:510] duration metric: took 4.0964629s for enable addons: enabled=[storage-provisioner default-storageclass]
I1027 10:26:05.315498    3660 start.go:246] waiting for cluster config update ...
I1027 10:26:05.315498    3660 start.go:255] writing updated cluster config ...
I1027 10:26:05.323117    3660 ssh_runner.go:195] Run: rm -f paused
I1027 10:26:05.860913    3660 start.go:600] kubectl: 1.31.1, cluster: 1.31.0 (minor skew: 0)
I1027 10:26:05.863107    3660 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.336221310Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.336272126Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.336285545Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.336605799Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:06 minikube cri-dockerd[1311]: time="2024-10-27T04:56:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ef16e9afc172efaa3fa1ceda33556e3fc7334c6a0993e8256a7be06ee0ebaa63/resolv.conf as [nameserver 172.28.144.1]"
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.538420681Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.538485049Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.538498408Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:06 minikube dockerd[1419]: time="2024-10-27T04:56:06.538586059Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.120217708Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.120424427Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.120462735Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.120536546Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube cri-dockerd[1311]: time="2024-10-27T04:56:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6576c5071516986949d8a31fae45bb11a4ddbaa089ff971a5f3e5f493687c1e2/resolv.conf as [nameserver 172.28.144.1]"
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.222333113Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.222452232Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.222488449Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.222628847Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.285460568Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.285592974Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.285635849Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.285725565Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube cri-dockerd[1311]: time="2024-10-27T04:56:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7552c8966a69d8ae54d6aeee2e69361a0f862ca73784af505853f6adc059d4a6/resolv.conf as [nameserver 172.28.144.1]"
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.494446669Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.494533764Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.494549377Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:07 minikube dockerd[1419]: time="2024-10-27T04:56:07.495078372Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:11 minikube cri-dockerd[1311]: time="2024-10-27T04:56:11Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.672939246Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.673021644Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.673042199Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.673137890Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.677524291Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.677575867Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.677584307Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:19 minikube dockerd[1419]: time="2024-10-27T04:56:19.677742024Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:19 minikube cri-dockerd[1311]: time="2024-10-27T04:56:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/96c921fdc3460aadfa4017db6a4df831c9c74bfde41b285c9b4d8be6b6c392fe/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 27 04:56:19 minikube cri-dockerd[1311]: time="2024-10-27T04:56:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/da3448f319438d76097f6675f4d602129ee6f44e402f7d0876db8e4e3142e4e0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 27 04:56:34 minikube cri-dockerd[1311]: time="2024-10-27T04:56:34Z" level=info msg="Pulling image roddakmr1996/kub-image:latest: 04f88f6c2caa: Downloading [============================>                      ]  10.61MB/18.71MB"
Oct 27 04:56:36 minikube dockerd[1411]: time="2024-10-27T04:56:36.598055681Z" level=info msg="ignoring event" container=d4533367072a460fea7b16488dcd0d6d975a99d163e2172a6348e55f8ca5a350 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 27 04:56:36 minikube dockerd[1419]: time="2024-10-27T04:56:36.599222197Z" level=info msg="shim disconnected" id=d4533367072a460fea7b16488dcd0d6d975a99d163e2172a6348e55f8ca5a350 namespace=moby
Oct 27 04:56:36 minikube dockerd[1419]: time="2024-10-27T04:56:36.599542532Z" level=warning msg="cleaning up after shim disconnected" id=d4533367072a460fea7b16488dcd0d6d975a99d163e2172a6348e55f8ca5a350 namespace=moby
Oct 27 04:56:36 minikube dockerd[1419]: time="2024-10-27T04:56:36.599668955Z" level=info msg="cleaning up dead shim" namespace=moby
Oct 27 04:56:37 minikube dockerd[1419]: time="2024-10-27T04:56:37.070882310Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:56:37 minikube dockerd[1419]: time="2024-10-27T04:56:37.070942859Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:56:37 minikube dockerd[1419]: time="2024-10-27T04:56:37.070960252Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:37 minikube dockerd[1419]: time="2024-10-27T04:56:37.071031260Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:56:44 minikube cri-dockerd[1311]: time="2024-10-27T04:56:44Z" level=info msg="Pulling image roddakmr1996/kub-image:latest: cea352090e12: Downloading [==========================>                        ]  17.04MB/32.24MB"
Oct 27 04:56:54 minikube cri-dockerd[1311]: time="2024-10-27T04:56:54Z" level=info msg="Pulling image roddakmr1996/kub-image:latest: cea352090e12: Downloading [=============================================>     ]  29.17MB/32.24MB"
Oct 27 04:57:04 minikube cri-dockerd[1311]: time="2024-10-27T04:57:04Z" level=info msg="Pulling image roddakmr1996/kub-image:latest: f11c1adaa26e: Downloading [================================================>  ]  28.08MB/29.13MB"
Oct 27 04:57:09 minikube cri-dockerd[1311]: time="2024-10-27T04:57:09Z" level=info msg="Stop pulling image roddakmr1996/kub-image:latest: Status: Downloaded newer image for roddakmr1996/kub-image:latest"
Oct 27 04:57:09 minikube dockerd[1419]: time="2024-10-27T04:57:09.836653378Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:57:09 minikube dockerd[1419]: time="2024-10-27T04:57:09.836724117Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:57:09 minikube dockerd[1419]: time="2024-10-27T04:57:09.836737265Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:57:09 minikube dockerd[1419]: time="2024-10-27T04:57:09.836793496Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:57:12 minikube cri-dockerd[1311]: time="2024-10-27T04:57:12Z" level=info msg="Stop pulling image roddakmr1996/kub-image:latest: Status: Image is up to date for roddakmr1996/kub-image:latest"
Oct 27 04:57:12 minikube dockerd[1419]: time="2024-10-27T04:57:12.937466890Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Oct 27 04:57:12 minikube dockerd[1419]: time="2024-10-27T04:57:12.938020856Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Oct 27 04:57:12 minikube dockerd[1419]: time="2024-10-27T04:57:12.938111898Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Oct 27 04:57:12 minikube dockerd[1419]: time="2024-10-27T04:57:12.938254651Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1


==> container status <==
CONTAINER           IMAGE                                                                                            CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
4c5233e0eb253       roddakmr1996/kub-image@sha256:503ca80d8955f0ce13293dfee8e7e0245731fc1d72e44b0d91bbc8513a51caca   5 minutes ago       Running             kubtest-container         0                   da3448f319438       kubtest-deployment-8594c7fcdd-5tjzc
8cfb56c00a8d3       roddakmr1996/kub-image@sha256:503ca80d8955f0ce13293dfee8e7e0245731fc1d72e44b0d91bbc8513a51caca   6 minutes ago       Running             kubtest-container         0                   96c921fdc3460       kubtest-deployment-8594c7fcdd-k8tkk
e6ea3e112869d       6e38f40d628db                                                                                    6 minutes ago       Running             storage-provisioner       1                   ef16e9afc172e       storage-provisioner
2866c9fcea87c       cbb01a7bd410d                                                                                    7 minutes ago       Running             coredns                   0                   7552c8966a69d       coredns-6f6b679f8f-lm2dm
6fe5c7a51652f       ad83b2ca7b09e                                                                                    7 minutes ago       Running             kube-proxy                0                   6576c50715169       kube-proxy-pxjm4
d4533367072a4       6e38f40d628db                                                                                    7 minutes ago       Exited              storage-provisioner       0                   ef16e9afc172e       storage-provisioner
f436a0d3a288c       1766f54c897f0                                                                                    7 minutes ago       Running             kube-scheduler            0                   7558efa992234       kube-scheduler-minikube
0d4582e62036f       2e96e5913fc06                                                                                    7 minutes ago       Running             etcd                      0                   05dc8dcffd36c       etcd-minikube
9c034e475e0e4       604f5db92eaa8                                                                                    7 minutes ago       Running             kube-apiserver            0                   b66f67a9fbc72       kube-apiserver-minikube
feb8f683e91eb       045733566833c                                                                                    7 minutes ago       Running             kube-controller-manager   0                   3737cf4cc31da       kube-controller-manager-minikube


==> coredns [2866c9fcea87] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e306d1e7ea9f136441adb86edb5191d0ad34db1b7fe2cc6e38c2f80d13fe074d8998e777c432ed4e98779dba207af803d1342f4bf1b04774b74d5ba7943dd60
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:50348 - 27595 "HINFO IN 1983072069700938997.3163449437150200329. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.258825707s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1122636294]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Oct-2024 04:56:07.548) (total time: 30000ms):
Trace[1122636294]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (04:56:37.549)
Trace[1122636294]: [30.000780277s] [30.000780277s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[487200555]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Oct-2024 04:56:07.549) (total time: 30000ms):
Trace[487200555]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (04:56:37.549)
Trace[487200555]: [30.000462993s] [30.000462993s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[149351432]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Oct-2024 04:56:07.549) (total time: 30000ms):
Trace[149351432]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (04:56:37.549)
Trace[149351432]: [30.000455448s] [30.000455448s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_10_27T10_26_01_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 27 Oct 2024 04:55:58 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 27 Oct 2024 05:03:10 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 27 Oct 2024 05:02:39 +0000   Sun, 27 Oct 2024 04:55:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 27 Oct 2024 05:02:39 +0000   Sun, 27 Oct 2024 04:55:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 27 Oct 2024 05:02:39 +0000   Sun, 27 Oct 2024 04:55:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 27 Oct 2024 05:02:39 +0000   Sun, 27 Oct 2024 04:56:02 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.28.146.145
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912872Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912872Ki
  pods:               110
System Info:
  Machine ID:                 91ec5bbef5c141daa9ada9c5e4d715b3
  System UUID:                4f29128b-e9e7-9e42-a756-694144fe89bb
  Boot ID:                    68b6f383-f2bd-4610-8c54-f0ac6a9864c0
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     kubtest-deployment-8594c7fcdd-5tjzc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m53s
  default                     kubtest-deployment-8594c7fcdd-k8tkk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m53s
  kube-system                 coredns-6f6b679f8f-lm2dm               100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     7m5s
  kube-system                 etcd-minikube                          100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         7m11s
  kube-system                 kube-apiserver-minikube                250m (12%)    0 (0%)      0 (0%)           0 (0%)         7m11s
  kube-system                 kube-controller-manager-minikube       200m (10%)    0 (0%)      0 (0%)           0 (0%)         7m10s
  kube-system                 kube-proxy-pxjm4                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m5s
  kube-system                 kube-scheduler-minikube                100m (5%)     0 (0%)      0 (0%)           0 (0%)         7m11s
  kube-system                 storage-provisioner                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m6s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age    From             Message
  ----    ------                   ----   ----             -------
  Normal  Starting                 7m4s   kube-proxy       
  Normal  Starting                 7m10s  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  7m10s  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  7m10s  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    7m10s  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     7m10s  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeReady                7m9s   kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           7m6s   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Oct27 04:54] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000000] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.048735] Spectre V2 : WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks possible via Spectre v2 BHB attacks!
[  +0.036963] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +0.006384] * Found PM-Timer Bug on the chipset. Due to workarounds for a bug,
              * this clock source is slow. Consider trying other clock sources
[Oct27 04:55] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +0.922012] psmouse serio1: trackpoint: failed to get extended button data, assuming 3 buttons
[  +0.442306] systemd-fstab-generator[115]: Ignoring "noauto" option for root device
[  +2.310654] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000009] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000002] NFSD: Unable to initialize client recovery tracking! (-2)
[ +24.064271] systemd-fstab-generator[634]: Ignoring "noauto" option for root device
[  +0.091246] systemd-fstab-generator[646]: Ignoring "noauto" option for root device
[ +15.670829] systemd-fstab-generator[985]: Ignoring "noauto" option for root device
[  +0.047867] kauditd_printk_skb: 65 callbacks suppressed
[  +0.386079] systemd-fstab-generator[1026]: Ignoring "noauto" option for root device
[  +0.088484] systemd-fstab-generator[1038]: Ignoring "noauto" option for root device
[  +0.093803] systemd-fstab-generator[1052]: Ignoring "noauto" option for root device
[  +2.480096] systemd-fstab-generator[1264]: Ignoring "noauto" option for root device
[  +0.121676] systemd-fstab-generator[1276]: Ignoring "noauto" option for root device
[  +0.087222] systemd-fstab-generator[1288]: Ignoring "noauto" option for root device
[  +0.126037] systemd-fstab-generator[1303]: Ignoring "noauto" option for root device
[  +2.754951] systemd-fstab-generator[1403]: Ignoring "noauto" option for root device
[  +0.049874] kauditd_printk_skb: 202 callbacks suppressed
[  +2.615361] systemd-fstab-generator[1652]: Ignoring "noauto" option for root device
[  +2.567977] systemd-fstab-generator[1759]: Ignoring "noauto" option for root device
[  +0.048707] kauditd_printk_skb: 70 callbacks suppressed
[Oct27 04:56] systemd-fstab-generator[2163]: Ignoring "noauto" option for root device
[  +0.057444] kauditd_printk_skb: 62 callbacks suppressed
[  +1.153029] systemd-fstab-generator[2222]: Ignoring "noauto" option for root device
[  +3.865069] kauditd_printk_skb: 34 callbacks suppressed
[ +13.347114] kauditd_printk_skb: 57 callbacks suppressed
[  +7.086614] kauditd_printk_skb: 14 callbacks suppressed


==> etcd [0d4582e62036] <==
{"level":"warn","ts":"2024-10-27T04:55:57.517007Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-10-27T04:55:57.517108Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://172.28.146.145:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://172.28.146.145:2380","--initial-cluster=minikube=https://172.28.146.145:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://172.28.146.145:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://172.28.146.145:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-10-27T04:55:57.517178Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-10-27T04:55:57.517202Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://172.28.146.145:2380"]}
{"level":"info","ts":"2024-10-27T04:55:57.517234Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-10-27T04:55:57.517710Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://172.28.146.145:2379"]}
{"level":"info","ts":"2024-10-27T04:55:57.517813Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://172.28.146.145:2380"],"listen-peer-urls":["https://172.28.146.145:2380"],"advertise-client-urls":["https://172.28.146.145:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.28.146.145:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://172.28.146.145:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-10-27T04:55:57.525563Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"7.549866ms"}
{"level":"info","ts":"2024-10-27T04:55:57.538069Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"b9a6758c19b5bdde","cluster-id":"e2806289b92afdd5"}
{"level":"info","ts":"2024-10-27T04:55:57.538212Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde switched to configuration voters=()"}
{"level":"info","ts":"2024-10-27T04:55:57.538267Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde became follower at term 0"}
{"level":"info","ts":"2024-10-27T04:55:57.538316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b9a6758c19b5bdde [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-10-27T04:55:57.538345Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde became follower at term 1"}
{"level":"info","ts":"2024-10-27T04:55:57.538416Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde switched to configuration voters=(13377508987737849310)"}
{"level":"warn","ts":"2024-10-27T04:55:57.560401Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-10-27T04:55:57.572639Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-10-27T04:55:57.584389Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-10-27T04:55:57.600006Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"b9a6758c19b5bdde","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-10-27T04:55:57.600503Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-27T04:55:57.619108Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b9a6758c19b5bdde","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-10-27T04:55:57.619280Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-27T04:55:57.619358Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-27T04:55:57.619402Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-27T04:55:57.637248Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-10-27T04:55:57.639500Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"b9a6758c19b5bdde","initial-advertise-peer-urls":["https://172.28.146.145:2380"],"listen-peer-urls":["https://172.28.146.145:2380"],"advertise-client-urls":["https://172.28.146.145:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.28.146.145:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-10-27T04:55:57.639569Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-10-27T04:55:57.639644Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"172.28.146.145:2380"}
{"level":"info","ts":"2024-10-27T04:55:57.639691Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"172.28.146.145:2380"}
{"level":"info","ts":"2024-10-27T04:55:57.649528Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde switched to configuration voters=(13377508987737849310)"}
{"level":"info","ts":"2024-10-27T04:55:57.649594Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"e2806289b92afdd5","local-member-id":"b9a6758c19b5bdde","added-peer-id":"b9a6758c19b5bdde","added-peer-peer-urls":["https://172.28.146.145:2380"]}
{"level":"info","ts":"2024-10-27T04:55:57.942393Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde is starting a new election at term 1"}
{"level":"info","ts":"2024-10-27T04:55:57.942507Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde became pre-candidate at term 1"}
{"level":"info","ts":"2024-10-27T04:55:57.942572Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde received MsgPreVoteResp from b9a6758c19b5bdde at term 1"}
{"level":"info","ts":"2024-10-27T04:55:57.942627Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde became candidate at term 2"}
{"level":"info","ts":"2024-10-27T04:55:57.942654Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde received MsgVoteResp from b9a6758c19b5bdde at term 2"}
{"level":"info","ts":"2024-10-27T04:55:57.942700Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b9a6758c19b5bdde became leader at term 2"}
{"level":"info","ts":"2024-10-27T04:55:57.942729Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b9a6758c19b5bdde elected leader b9a6758c19b5bdde at term 2"}
{"level":"info","ts":"2024-10-27T04:55:57.952416Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-27T04:55:57.956481Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"b9a6758c19b5bdde","local-member-attributes":"{Name:minikube ClientURLs:[https://172.28.146.145:2379]}","request-path":"/0/members/b9a6758c19b5bdde/attributes","cluster-id":"e2806289b92afdd5","publish-timeout":"7s"}
{"level":"info","ts":"2024-10-27T04:55:57.956642Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-27T04:55:57.956893Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-27T04:55:57.957042Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-10-27T04:55:57.957079Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-10-27T04:55:57.957781Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-27T04:55:57.959117Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-27T04:55:57.961402Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"172.28.146.145:2379"}
{"level":"info","ts":"2024-10-27T04:55:57.961502Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-10-27T04:55:57.961750Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"e2806289b92afdd5","local-member-id":"b9a6758c19b5bdde","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-27T04:55:57.961791Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-27T04:55:57.961826Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}


==> kernel <==
 05:03:11 up 8 min,  0 users,  load average: 0.12, 0.27, 0.18
Linux minikube 5.10.207 #1 SMP Tue Sep 3 21:45:30 UTC 2024 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [9c034e475e0e] <==
I1027 04:55:58.794797       1 controller.go:78] Starting OpenAPI AggregationController
I1027 04:55:58.794869       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1027 04:55:58.794901       1 aggregator.go:169] waiting for initial CRD sync...
I1027 04:55:58.794931       1 controller.go:119] Starting legacy_token_tracking_controller
I1027 04:55:58.794935       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1027 04:55:58.794944       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1027 04:55:58.794948       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1027 04:55:58.795582       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1027 04:55:58.795632       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1027 04:55:58.795818       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1027 04:55:58.795934       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1027 04:55:58.795963       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1027 04:55:58.796107       1 local_available_controller.go:156] Starting LocalAvailability controller
I1027 04:55:58.796149       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1027 04:55:58.797110       1 controller.go:142] Starting OpenAPI controller
I1027 04:55:58.797128       1 controller.go:90] Starting OpenAPI V3 controller
I1027 04:55:58.797940       1 naming_controller.go:294] Starting NamingConditionController
I1027 04:55:58.797956       1 establishing_controller.go:81] Starting EstablishingController
I1027 04:55:58.797963       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1027 04:55:58.797970       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1027 04:55:58.797975       1 crd_finalizer.go:269] Starting CRDFinalizer
I1027 04:55:58.798149       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1027 04:55:58.798153       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1027 04:55:58.798903       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1027 04:55:58.798983       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1027 04:55:58.880537       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1027 04:55:58.880571       1 policy_source.go:224] refreshing policies
I1027 04:55:58.880542       1 shared_informer.go:320] Caches are synced for node_authorizer
E1027 04:55:58.888078       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1027 04:55:58.895213       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1027 04:55:58.895489       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1027 04:55:58.895662       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1027 04:55:58.895679       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1027 04:55:58.895752       1 shared_informer.go:320] Caches are synced for configmaps
I1027 04:55:58.895955       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1027 04:55:58.896047       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1027 04:55:58.896183       1 cache.go:39] Caches are synced for LocalAvailability controller
I1027 04:55:58.896954       1 controller.go:615] quota admission added evaluator for: namespaces
I1027 04:55:58.898190       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1027 04:55:58.898464       1 aggregator.go:171] initial CRD sync complete...
I1027 04:55:58.898523       1 autoregister_controller.go:144] Starting autoregister controller
I1027 04:55:58.898577       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1027 04:55:58.898622       1 cache.go:39] Caches are synced for autoregister controller
I1027 04:55:59.091337       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1027 04:55:59.806692       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1027 04:55:59.816544       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1027 04:55:59.817276       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1027 04:56:00.654681       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1027 04:56:00.704696       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1027 04:56:00.820979       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
I1027 04:56:00.845591       1 controller.go:615] quota admission added evaluator for: serviceaccounts
W1027 04:56:00.860964       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [172.28.146.145]
I1027 04:56:00.862642       1 controller.go:615] quota admission added evaluator for: endpoints
I1027 04:56:00.875429       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1027 04:56:01.643740       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1027 04:56:01.665713       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1027 04:56:01.676072       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1027 04:56:06.388705       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1027 04:56:06.580143       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1027 04:56:26.784003       1 alloc.go:330] "allocated clusterIPs" service="default/kubtest-service" clusterIPs={"IPv4":"10.108.195.144"}


==> kube-controller-manager [feb8f683e91e] <==
I1027 04:56:05.573939       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1027 04:56:05.579441       1 shared_informer.go:320] Caches are synced for HPA
I1027 04:56:05.579707       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1027 04:56:05.582263       1 shared_informer.go:320] Caches are synced for ReplicationController
I1027 04:56:05.582747       1 shared_informer.go:320] Caches are synced for job
I1027 04:56:05.584579       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1027 04:56:05.586205       1 shared_informer.go:320] Caches are synced for stateful set
I1027 04:56:05.587548       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1027 04:56:05.587804       1 shared_informer.go:320] Caches are synced for service account
I1027 04:56:05.589085       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1027 04:56:05.589106       1 shared_informer.go:320] Caches are synced for PV protection
I1027 04:56:05.589120       1 shared_informer.go:320] Caches are synced for expand
I1027 04:56:05.589144       1 shared_informer.go:320] Caches are synced for deployment
I1027 04:56:05.592248       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1027 04:56:05.592280       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1027 04:56:05.606028       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1027 04:56:05.631585       1 shared_informer.go:320] Caches are synced for TTL
I1027 04:56:05.632228       1 shared_informer.go:320] Caches are synced for node
I1027 04:56:05.632258       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1027 04:56:05.632270       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1027 04:56:05.632273       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1027 04:56:05.632282       1 shared_informer.go:320] Caches are synced for cidrallocator
I1027 04:56:05.634712       1 shared_informer.go:320] Caches are synced for attach detach
I1027 04:56:05.651203       1 shared_informer.go:320] Caches are synced for GC
I1027 04:56:05.651653       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1027 04:56:05.651705       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1027 04:56:05.651732       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1027 04:56:05.670575       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1027 04:56:05.683622       1 shared_informer.go:320] Caches are synced for persistent volume
I1027 04:56:05.733404       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1027 04:56:05.734794       1 shared_informer.go:320] Caches are synced for cronjob
I1027 04:56:05.786756       1 shared_informer.go:320] Caches are synced for resource quota
I1027 04:56:05.789673       1 shared_informer.go:320] Caches are synced for resource quota
I1027 04:56:05.830162       1 shared_informer.go:320] Caches are synced for taint
I1027 04:56:05.830565       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1027 04:56:05.830762       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1027 04:56:05.830900       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1027 04:56:05.831596       1 shared_informer.go:320] Caches are synced for daemon sets
I1027 04:56:05.847579       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1027 04:56:06.218464       1 shared_informer.go:320] Caches are synced for garbage collector
I1027 04:56:06.276507       1 shared_informer.go:320] Caches are synced for garbage collector
I1027 04:56:06.276536       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1027 04:56:06.504193       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1027 04:56:06.725462       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="327.073097ms"
I1027 04:56:06.744408       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="15.938143ms"
I1027 04:56:06.744537       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="43.421µs"
I1027 04:56:06.744682       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="67.276µs"
I1027 04:56:07.678219       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="67.732µs"
I1027 04:56:11.678429       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1027 04:56:18.205595       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubtest-deployment-8594c7fcdd" duration="79.031827ms"
I1027 04:56:18.247145       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubtest-deployment-8594c7fcdd" duration="41.298674ms"
I1027 04:56:18.247301       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubtest-deployment-8594c7fcdd" duration="39.999µs"
I1027 04:56:44.180904       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="26.321379ms"
I1027 04:56:44.181163       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="59.972µs"
I1027 04:57:10.224094       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubtest-deployment-8594c7fcdd" duration="12.358876ms"
I1027 04:57:10.224899       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubtest-deployment-8594c7fcdd" duration="32.825µs"
I1027 04:57:13.241317       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubtest-deployment-8594c7fcdd" duration="8.918002ms"
I1027 04:57:13.241369       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kubtest-deployment-8594c7fcdd" duration="18.269µs"
I1027 04:57:33.865374       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1027 05:02:39.771837       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [6fe5c7a51652] <==
I1027 04:56:07.366534       1 server_linux.go:66] "Using iptables proxy"
E1027 04:56:07.378408       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E1027 04:56:07.386567       1 proxier.go:734] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I1027 04:56:07.392722       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.28.146.145"]
E1027 04:56:07.392815       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1027 04:56:07.409494       1 server_linux.go:146] "No iptables support for family" ipFamily="IPv6"
I1027 04:56:07.409543       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I1027 04:56:07.409559       1 server_linux.go:169] "Using iptables Proxier"
I1027 04:56:07.410795       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1027 04:56:07.410951       1 server.go:483] "Version info" version="v1.31.0"
I1027 04:56:07.410961       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1027 04:56:07.411610       1 config.go:197] "Starting service config controller"
I1027 04:56:07.411626       1 shared_informer.go:313] Waiting for caches to sync for service config
I1027 04:56:07.411637       1 config.go:104] "Starting endpoint slice config controller"
I1027 04:56:07.411640       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1027 04:56:07.412179       1 config.go:326] "Starting node config controller"
I1027 04:56:07.412189       1 shared_informer.go:313] Waiting for caches to sync for node config
I1027 04:56:07.512033       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1027 04:56:07.512078       1 shared_informer.go:320] Caches are synced for service config
I1027 04:56:07.512396       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [f436a0d3a288] <==
I1027 04:55:58.839445       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W1027 04:55:58.847439       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1027 04:55:58.847464       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1027 04:55:58.847520       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1027 04:55:58.847533       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.847614       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1027 04:55:58.847660       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.847741       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1027 04:55:58.847770       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1027 04:55:58.847779       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.847804       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1027 04:55:58.847812       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1027 04:55:58.847805       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.847874       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1027 04:55:58.847883       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.847907       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1027 04:55:58.847917       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.848179       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1027 04:55:58.848239       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.848294       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1027 04:55:58.848327       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.848624       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1027 04:55:58.848669       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.848747       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1027 04:55:58.848780       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.848806       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1027 04:55:58.848840       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.848891       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1027 04:55:58.848928       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:58.848999       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1027 04:55:58.849037       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.663056       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1027 04:55:59.663219       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.686628       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1027 04:55:59.686666       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.711647       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1027 04:55:59.711698       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.722052       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1027 04:55:59.722335       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.804091       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1027 04:55:59.804168       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.850205       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1027 04:55:59.850274       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1027 04:55:59.851869       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1027 04:55:59.851951       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.854121       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1027 04:55:59.854155       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1027 04:55:59.976983       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1027 04:55:59.977345       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:56:00.096487       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1027 04:56:00.096729       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1027 04:56:00.140199       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1027 04:56:00.140421       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:56:00.272852       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1027 04:56:00.272896       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1027 04:56:00.301301       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1027 04:56:00.301462       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1027 04:56:00.334951       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1027 04:56:00.334988       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
I1027 04:56:02.439727       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Oct 27 04:56:02 minikube kubelet[2170]: I1027 04:56:02.604240    2170 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=2.604235948 podStartE2EDuration="2.604235948s" podCreationTimestamp="2024-10-27 04:56:00 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-10-27 04:56:02.593220143 +0000 UTC m=+1.214524450" watchObservedRunningTime="2024-10-27 04:56:02.604235948 +0000 UTC m=+1.225540253"
Oct 27 04:56:02 minikube kubelet[2170]: I1027 04:56:02.888487    2170 kubelet_node_status.go:488] "Fast updating node status as it just became ready"
Oct 27 04:56:05 minikube kubelet[2170]: I1027 04:56:05.903789    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6d5db\" (UniqueName: \"kubernetes.io/projected/c6222336-7a3b-4e54-9ed7-74ce8efc1d02-kube-api-access-6d5db\") pod \"storage-provisioner\" (UID: \"c6222336-7a3b-4e54-9ed7-74ce8efc1d02\") " pod="kube-system/storage-provisioner"
Oct 27 04:56:05 minikube kubelet[2170]: I1027 04:56:05.903830    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/c6222336-7a3b-4e54-9ed7-74ce8efc1d02-tmp\") pod \"storage-provisioner\" (UID: \"c6222336-7a3b-4e54-9ed7-74ce8efc1d02\") " pod="kube-system/storage-provisioner"
Oct 27 04:56:06 minikube kubelet[2170]: I1027 04:56:06.009542    2170 swap_util.go:74] "error creating dir to test if tmpfs noswap is enabled. Assuming not supported" mount path="" error="stat /var/lib/kubelet/plugins/kubernetes.io/empty-dir: no such file or directory"
Oct 27 04:56:06 minikube kubelet[2170]: I1027 04:56:06.709774    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9mcxp\" (UniqueName: \"kubernetes.io/projected/93ae3214-7cf3-4d14-83ed-499ebd1fada3-kube-api-access-9mcxp\") pod \"kube-proxy-pxjm4\" (UID: \"93ae3214-7cf3-4d14-83ed-499ebd1fada3\") " pod="kube-system/kube-proxy-pxjm4"
Oct 27 04:56:06 minikube kubelet[2170]: I1027 04:56:06.709870    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/93ae3214-7cf3-4d14-83ed-499ebd1fada3-kube-proxy\") pod \"kube-proxy-pxjm4\" (UID: \"93ae3214-7cf3-4d14-83ed-499ebd1fada3\") " pod="kube-system/kube-proxy-pxjm4"
Oct 27 04:56:06 minikube kubelet[2170]: I1027 04:56:06.709895    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/93ae3214-7cf3-4d14-83ed-499ebd1fada3-xtables-lock\") pod \"kube-proxy-pxjm4\" (UID: \"93ae3214-7cf3-4d14-83ed-499ebd1fada3\") " pod="kube-system/kube-proxy-pxjm4"
Oct 27 04:56:06 minikube kubelet[2170]: I1027 04:56:06.709921    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/93ae3214-7cf3-4d14-83ed-499ebd1fada3-lib-modules\") pod \"kube-proxy-pxjm4\" (UID: \"93ae3214-7cf3-4d14-83ed-499ebd1fada3\") " pod="kube-system/kube-proxy-pxjm4"
Oct 27 04:56:06 minikube kubelet[2170]: I1027 04:56:06.810647    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tzb9j\" (UniqueName: \"kubernetes.io/projected/e144400c-e75e-4d8f-87f6-ed14373c59c5-kube-api-access-tzb9j\") pod \"coredns-6f6b679f8f-lm2dm\" (UID: \"e144400c-e75e-4d8f-87f6-ed14373c59c5\") " pod="kube-system/coredns-6f6b679f8f-lm2dm"
Oct 27 04:56:06 minikube kubelet[2170]: I1027 04:56:06.810945    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/e144400c-e75e-4d8f-87f6-ed14373c59c5-config-volume\") pod \"coredns-6f6b679f8f-lm2dm\" (UID: \"e144400c-e75e-4d8f-87f6-ed14373c59c5\") " pod="kube-system/coredns-6f6b679f8f-lm2dm"
Oct 27 04:56:07 minikube kubelet[2170]: I1027 04:56:07.635036    2170 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=2.635010957 podStartE2EDuration="2.635010957s" podCreationTimestamp="2024-10-27 04:56:05 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-10-27 04:56:07.634952179 +0000 UTC m=+6.256256530" watchObservedRunningTime="2024-10-27 04:56:07.635010957 +0000 UTC m=+6.256315297"
Oct 27 04:56:07 minikube kubelet[2170]: I1027 04:56:07.676841    2170 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-pxjm4" podStartSLOduration=1.6768160779999999 podStartE2EDuration="1.676816078s" podCreationTimestamp="2024-10-27 04:56:06 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-10-27 04:56:07.65140491 +0000 UTC m=+6.272709240" watchObservedRunningTime="2024-10-27 04:56:07.676816078 +0000 UTC m=+6.298120416"
Oct 27 04:56:08 minikube kubelet[2170]: I1027 04:56:08.970338    2170 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-6f6b679f8f-lm2dm" podStartSLOduration=2.970316622 podStartE2EDuration="2.970316622s" podCreationTimestamp="2024-10-27 04:56:06 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-10-27 04:56:07.67759432 +0000 UTC m=+6.298898662" watchObservedRunningTime="2024-10-27 04:56:08.970316622 +0000 UTC m=+7.591620956"
Oct 27 04:56:11 minikube kubelet[2170]: I1027 04:56:11.657475    2170 kuberuntime_manager.go:1633] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 27 04:56:11 minikube kubelet[2170]: I1027 04:56:11.659825    2170 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 27 04:56:14 minikube kubelet[2170]: I1027 04:56:14.128575    2170 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 27 04:56:18 minikube kubelet[2170]: W1027 04:56:18.186570    2170 reflector.go:561] object-"default"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "default": no relationship found between node 'minikube' and this object
Oct 27 04:56:18 minikube kubelet[2170]: E1027 04:56:18.186881    2170 reflector.go:158] "Unhandled Error" err="object-\"default\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:minikube\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\": no relationship found between node 'minikube' and this object" logger="UnhandledError"
Oct 27 04:56:18 minikube kubelet[2170]: I1027 04:56:18.194870    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-t5dhx\" (UniqueName: \"kubernetes.io/projected/58f2d33c-1104-44c6-95dc-1082feed6977-kube-api-access-t5dhx\") pod \"kubtest-deployment-8594c7fcdd-k8tkk\" (UID: \"58f2d33c-1104-44c6-95dc-1082feed6977\") " pod="default/kubtest-deployment-8594c7fcdd-k8tkk"
Oct 27 04:56:18 minikube kubelet[2170]: I1027 04:56:18.295418    2170 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x27fd\" (UniqueName: \"kubernetes.io/projected/ebfde21e-bd73-4ff3-be4f-7641e8b9afca-kube-api-access-x27fd\") pod \"kubtest-deployment-8594c7fcdd-5tjzc\" (UID: \"ebfde21e-bd73-4ff3-be4f-7641e8b9afca\") " pod="default/kubtest-deployment-8594c7fcdd-5tjzc"
Oct 27 04:56:19 minikube kubelet[2170]: I1027 04:56:19.807538    2170 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="da3448f319438d76097f6675f4d602129ee6f44e402f7d0876db8e4e3142e4e0"
Oct 27 04:56:19 minikube kubelet[2170]: I1027 04:56:19.810298    2170 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="96c921fdc3460aadfa4017db6a4df831c9c74bfde41b285c9b4d8be6b6c392fe"
Oct 27 04:56:36 minikube kubelet[2170]: I1027 04:56:36.954856    2170 scope.go:117] "RemoveContainer" containerID="d4533367072a460fea7b16488dcd0d6d975a99d163e2172a6348e55f8ca5a350"
Oct 27 04:57:01 minikube kubelet[2170]: E1027 04:57:01.497217    2170 iptables.go:577] "Could not set up iptables canary" err=<
Oct 27 04:57:01 minikube kubelet[2170]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 27 04:57:01 minikube kubelet[2170]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 27 04:57:01 minikube kubelet[2170]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 27 04:57:01 minikube kubelet[2170]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 27 04:57:13 minikube kubelet[2170]: I1027 04:57:13.231702    2170 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/kubtest-deployment-8594c7fcdd-k8tkk" podStartSLOduration=5.365520496 podStartE2EDuration="55.231691105s" podCreationTimestamp="2024-10-27 04:56:18 +0000 UTC" firstStartedPulling="2024-10-27 04:56:19.83422417 +0000 UTC m=+18.455528466" lastFinishedPulling="2024-10-27 04:57:09.700394769 +0000 UTC m=+68.321699075" observedRunningTime="2024-10-27 04:57:10.211340643 +0000 UTC m=+68.832644966" watchObservedRunningTime="2024-10-27 04:57:13.231691105 +0000 UTC m=+71.852995447"
Oct 27 04:58:01 minikube kubelet[2170]: E1027 04:58:01.494348    2170 iptables.go:577] "Could not set up iptables canary" err=<
Oct 27 04:58:01 minikube kubelet[2170]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 27 04:58:01 minikube kubelet[2170]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 27 04:58:01 minikube kubelet[2170]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 27 04:58:01 minikube kubelet[2170]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 27 04:59:01 minikube kubelet[2170]: E1027 04:59:01.493071    2170 iptables.go:577] "Could not set up iptables canary" err=<
Oct 27 04:59:01 minikube kubelet[2170]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 27 04:59:01 minikube kubelet[2170]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 27 04:59:01 minikube kubelet[2170]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 27 04:59:01 minikube kubelet[2170]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 27 05:00:01 minikube kubelet[2170]: E1027 05:00:01.499033    2170 iptables.go:577] "Could not set up iptables canary" err=<
Oct 27 05:00:01 minikube kubelet[2170]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 27 05:00:01 minikube kubelet[2170]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 27 05:00:01 minikube kubelet[2170]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 27 05:00:01 minikube kubelet[2170]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 27 05:01:01 minikube kubelet[2170]: E1027 05:01:01.494939    2170 iptables.go:577] "Could not set up iptables canary" err=<
Oct 27 05:01:01 minikube kubelet[2170]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 27 05:01:01 minikube kubelet[2170]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 27 05:01:01 minikube kubelet[2170]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 27 05:01:01 minikube kubelet[2170]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 27 05:02:01 minikube kubelet[2170]: E1027 05:02:01.494818    2170 iptables.go:577] "Could not set up iptables canary" err=<
Oct 27 05:02:01 minikube kubelet[2170]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 27 05:02:01 minikube kubelet[2170]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 27 05:02:01 minikube kubelet[2170]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 27 05:02:01 minikube kubelet[2170]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Oct 27 05:03:01 minikube kubelet[2170]: E1027 05:03:01.492259    2170 iptables.go:577] "Could not set up iptables canary" err=<
Oct 27 05:03:01 minikube kubelet[2170]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Oct 27 05:03:01 minikube kubelet[2170]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Oct 27 05:03:01 minikube kubelet[2170]:         Perhaps ip6tables or your kernel needs to be upgraded.
Oct 27 05:03:01 minikube kubelet[2170]:  > table="nat" chain="KUBE-KUBELET-CANARY"


==> storage-provisioner [d4533367072a] <==
I1027 04:56:06.575447       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1027 04:56:36.578201       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [e6ea3e112869] <==
I1027 04:56:37.111468       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1027 04:56:37.120161       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1027 04:56:37.120202       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1027 04:56:37.128659       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1027 04:56:37.128806       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_28769554-44d9-4450-9562-1237ea82a594!
I1027 04:56:37.130661       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a079b763-1bf6-4b3f-ad4c-ae790c97efab", APIVersion:"v1", ResourceVersion:"408", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_28769554-44d9-4450-9562-1237ea82a594 became leader
I1027 04:56:37.230084       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_28769554-44d9-4450-9562-1237ea82a594!

